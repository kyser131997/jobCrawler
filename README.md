# üîç Job Crawler - Scraping d'Offres Data France

Application Streamlit compl√®te pour scraper automatiquement des offres d'emploi data en France, avec interface dark mode moderne, dashboard statistiques et tableau interactif.

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.31+-red.svg)
![Playwright](https://img.shields.io/badge/Playwright-1.41+-green.svg)

## üìã Table des Mati√®res

- [Fonctionnalit√©s](#-fonctionnalit√©s)
- [Stack Technique](#-stack-technique)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Architecture](#-architecture)
- [Ajouter une Nouvelle Source](#-ajouter-une-nouvelle-source)
- [Configuration](#-configuration)
- [D√©pannage](#-d√©pannage)

## ‚ú® Fonctionnalit√©s

### Scraping Intelligent
- ‚úÖ **Multi-sources** : Indeed, Welcome to the Jungle, LinkedIn
- ‚úÖ **Filtrage automatique** : Offres publi√©es dans les 3 derniers jours
- ‚úÖ **G√©olocalisation** : Uniquement France (villes, r√©gions, remote)
- ‚úÖ **D√©tection de r√¥les** : Data Analyst, Business Analyst, Data Engineer
- ‚úÖ **D√©duplication** : √âvite les doublons par URL ou hash

### Interface Moderne
- üé® **Dark Mode** : Interface √©l√©gante avec fond noir
- üìä **Dashboard Statistiques** : M√©triques, graphiques interactifs
- üîç **Tableau Filtrable** : Recherche, tri, filtres par cat√©gorie/source
- üì• **Export CSV** : T√©l√©chargement des r√©sultats
- üöÄ **Logs en Temps R√©el** : Suivi du scraping en direct

### Robustesse
- üîÑ **Retry Logic** : R√©essais automatiques en cas d'√©chec
- üõ°Ô∏è **Anti-D√©tection** : User agents rotatifs, d√©lais al√©atoires
- ‚ö†Ô∏è **Gestion d'Erreurs** : Continue m√™me si une source √©choue
- üíæ **Base SQLite** : Stockage persistant local

## üõ†Ô∏è Stack Technique

- **Frontend** : Streamlit (interface web)
- **Scraping** : Playwright (navigateur headless) + BeautifulSoup
- **Base de Donn√©es** : SQLite (fichier local)
- **Data Processing** : pandas, SQLAlchemy
- **Visualisation** : Plotly
- **Parsing Dates** : python-dateutil

## üì¶ Installation

### Pr√©requis
- Python 3.9 ou sup√©rieur
- pip (gestionnaire de paquets Python)

### √âtapes

1. **Cloner ou t√©l√©charger le projet**
```bash
cd JobCrawler
```

2. **Cr√©er un environnement virtuel**
```bash
python -m venv .venv
```

3. **Activer l'environnement virtuel**

Windows :
```bash
.venv\Scripts\activate
```

macOS/Linux :
```bash
source .venv/bin/activate
```

4. **Installer les d√©pendances**
```bash
pip install -r requirements.txt
```

5. **Installer les navigateurs Playwright**
```bash
playwright install chromium
```

## üöÄ Utilisation

### Lancer l'application

```bash
streamlit run app.py
```

L'application s'ouvrira automatiquement dans votre navigateur √† l'adresse `http://localhost:8501`.

### Workflow

1. **Cliquer sur "üöÄ Craquer les offres"**
   - Le scraping d√©marre automatiquement
   - Les logs s'affichent en temps r√©el
   - Dur√©e : 2-5 minutes selon les sources

2. **Consulter les statistiques**
   - Total d'offres
   - R√©partition par cat√©gorie (Data Analyst, Business Analyst, Data Engineer)
   - R√©partition par source
   - √âvolution par jour
   - Top localisations

3. **Explorer le tableau**
   - Filtrer par cat√©gorie, source
   - Rechercher par titre ou entreprise
   - Cliquer sur les URLs pour voir les offres

4. **Exporter les r√©sultats**
   - Bouton "üì• Exporter en CSV"
   - Fichier t√©l√©charg√© avec toutes les offres filtr√©es

## üèóÔ∏è Architecture

```
JobCrawler/
‚îú‚îÄ‚îÄ app.py                      # Application Streamlit principale
‚îú‚îÄ‚îÄ config.py                   # Configuration centralis√©e
‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances Python
‚îú‚îÄ‚îÄ jobs.db                     # Base de donn√©es SQLite (cr√©√©e automatiquement)
‚îú‚îÄ‚îÄ scraper/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ db.py                   # Mod√®les SQLAlchemy et gestion DB
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                # Fonctions utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py             # Orchestrateur principal
‚îÇ   ‚îî‚îÄ‚îÄ sources/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ base.py             # Interface abstraite SourceScraper
‚îÇ       ‚îú‚îÄ‚îÄ indeed_scraper.py   # Scraper Indeed France
‚îÇ       ‚îú‚îÄ‚îÄ wttj_scraper.py     # Scraper Welcome to the Jungle
‚îÇ       ‚îî‚îÄ‚îÄ linkedin_scraper.py # Scraper LinkedIn Jobs
‚îî‚îÄ‚îÄ README.md
```

### Flux de Donn√©es

```
[Sources Web] ‚Üí [Scrapers] ‚Üí [Pipeline] ‚Üí [Filtres] ‚Üí [Enrichissement] ‚Üí [SQLite] ‚Üí [Streamlit UI]
```

1. **Scrapers** : Chaque source (Indeed, WTTJ, LinkedIn) extrait les offres
2. **Pipeline** : Orchestre les scrapers, collecte les donn√©es brutes
3. **Filtres** : Applique les crit√®res (France, 3 jours, mots-cl√©s)
4. **Enrichissement** : Cat√©gorise les r√¥les, d√©tecte les mots-cl√©s
5. **SQLite** : Stocke avec d√©duplication
6. **Streamlit** : Affiche dashboard + tableau

## üîß Ajouter une Nouvelle Source

### √âtape 1 : Cr√©er le Scraper

Cr√©ez un fichier `scraper/sources/nouvelle_source.py` :

```python
from typing import List, Dict, Optional, Callable
from playwright.sync_api import Browser
from scraper.sources.base import SourceScraper
from scraper.utils import parse_relative_date, clean_text, extract_snippet, normalize_url

class NouvelleSourceScraper(SourceScraper):
    """Scraper pour Nouvelle Source."""
    
    def __init__(self):
        super().__init__("NouvelleSource")
        self.base_url = "https://example.com"
    
    def scrape(self, browser: Browser, progress_callback: Optional[Callable] = None) -> List[Dict]:
        """Scrappe les offres."""
        jobs = []
        page = self._create_page(browser)
        
        try:
            # 1. Naviguer vers la page
            page.goto(f"{self.base_url}/jobs?q=data&location=France", timeout=30000)
            
            # 2. Attendre le chargement
            page.wait_for_selector('.job-card', timeout=10000)
            
            # 3. Extraire les offres
            job_cards = page.query_selector_all('.job-card')
            
            for card in job_cards:
                # Extraire les donn√©es
                title = self._safe_get_text(card.query_selector('.title'))
                company = self._safe_get_text(card.query_selector('.company'))
                location = self._safe_get_text(card.query_selector('.location'))
                url = self._safe_get_attribute(card.query_selector('a'), 'href')
                
                jobs.append({
                    'job_title': clean_text(title),
                    'company': clean_text(company),
                    'location': clean_text(location),
                    'url': normalize_url(url),
                    'published_date': None,  # Parser si disponible
                    'snippet': '',
                    'source': self.source_name
                })
        
        finally:
            page.close()
        
        return jobs
```

### √âtape 2 : Enregistrer dans le Pipeline

Modifiez `scraper/pipeline.py` :

```python
from scraper.sources.nouvelle_source import NouvelleSourceScraper

class ScrapingPipeline:
    def __init__(self):
        self.db = DatabaseManager()
        self.sources = [
            IndeedScraper(),
            WTTJScraper(),
            LinkedInScraper(),
            NouvelleSourceScraper()  # ‚Üê Ajouter ici
        ]
```

### √âtape 3 : Tester

Relancez l'application et cliquez sur "Craquer les offres". La nouvelle source sera automatiquement scrap√©e.

## ‚öôÔ∏è Configuration

Modifiez `config.py` pour personnaliser :

### Mots-cl√©s
```python
KEYWORDS = {
    'data_analyst': ['data analyst', 'analyste de donn√©es'],
    'business_analyst': ['business analyst', 'analyste business'],
    'data_engineer': ['data engineer', 'ing√©nieur donn√©es'],
    # Ajouter vos mots-cl√©s
}
```

### Fen√™tre Temporelle
```python
MAX_DAYS_OLD = 3  # Modifier pour 7 jours, etc.
```

### Mode Headless
```python
HEADLESS = True  # False pour voir le navigateur
```

### Localisations
```python
FRENCH_LOCATIONS = [
    'france', 'paris', 'lyon', 'marseille',
    # Ajouter vos villes
]
```

## üêõ D√©pannage

### Erreur : "playwright not found"
```bash
playwright install chromium
```

### Erreur : "Module not found"
V√©rifiez que l'environnement virtuel est activ√© :
```bash
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # macOS/Linux
```

### Aucune offre trouv√©e
- V√©rifiez votre connexion Internet
- Certaines sources peuvent bloquer le scraping (captcha, 403)
- Essayez avec `HEADLESS = False` dans `config.py` pour d√©boguer

### Base de donn√©es corrompue
Supprimez `jobs.db` et relancez l'application :
```bash
rm jobs.db  # macOS/Linux
del jobs.db  # Windows
```

### Scraping tr√®s lent
- R√©duisez `REQUEST_DELAY_MAX` dans `config.py`
- Commentez certaines sources dans `pipeline.py`

## üìä Donn√©es Stock√©es

Chaque offre contient :
- **job_title** : Titre de l'offre
- **company** : Entreprise
- **role_category** : Data Analyst / Business Analyst / Data Engineer / Other
- **source** : Indeed / WTTJ / LinkedIn
- **published_date** : Date de publication (ISO)
- **location** : Ville/r√©gion/remote
- **url** : Lien direct (cl√© unique)
- **snippet** : R√©sum√© court
- **detected_keywords** : Mots-cl√©s d√©tect√©s
- **scraped_at** : Timestamp de scraping

## üîí Consid√©rations L√©gales

‚ö†Ô∏è **Important** : Ce projet est √† usage √©ducatif et personnel.

- Respectez les conditions d'utilisation des sites scrap√©s
- Ne surchargez pas les serveurs (d√©lais entre requ√™tes)
- Certains sites interdisent le scraping automatis√©
- Utilisez de mani√®re responsable et √©thique

## üìù Licence

Ce projet est fourni "tel quel" sans garantie. Utilisez-le √† vos propres risques.

## ü§ù Contribution

Pour am√©liorer le projet :
1. Ajoutez de nouvelles sources (voir section ci-dessus)
2. Am√©liorez les s√©lecteurs CSS (les sites changent r√©guli√®rement)
3. Optimisez les performances
4. Ajoutez des tests unitaires

## üìß Support

En cas de probl√®me :
1. V√©rifiez la section [D√©pannage](#-d√©pannage)
2. Consultez les logs dans l'interface Streamlit
3. V√©rifiez que les d√©pendances sont √† jour

---

**Bon scraping ! üöÄ**
